{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37cd8693",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-12T15:05:54.181897Z",
     "iopub.status.busy": "2024-09-12T15:05:54.180697Z",
     "iopub.status.idle": "2024-09-12T15:05:54.663315Z",
     "shell.execute_reply": "2024-09-12T15:05:54.661964Z"
    },
    "papermill": {
     "duration": 0.490671,
     "end_time": "2024-09-12T15:05:54.666008",
     "exception": false,
     "start_time": "2024-09-12T15:05:54.175337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/hr-data/HR_comma_sep.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328f378",
   "metadata": {
    "papermill": {
     "duration": 0.002356,
     "end_time": "2024-09-12T15:05:54.671510",
     "exception": false,
     "start_time": "2024-09-12T15:05:54.669154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## 1. Introduction\n",
    "- **Logistic Regression** is a supervised learning algorithm used for binary classification problems (where the target variable has two possible outcomes, such as 0 or 1).\n",
    "- Unlike **Linear Regression**, which predicts continuous values, **Logistic Regression** predicts the probability of a certain class or event, such as pass/fail, win/loss, or spam/ham.\n",
    "\n",
    "## 2. Logistic Function (Sigmoid Function)\n",
    "- The key difference between **Logistic Regression** and **Linear Regression** is the use of the **logistic function** (also called the **sigmoid function**) to predict probabilities.\n",
    "\n",
    "### **Sigmoid Function:**\n",
    "$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $z$ = the input to the function, which is a linear combination of the input features and their coefficients ($z = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\dots + \\beta_n \\cdot x_n$).\n",
    "- $e$ = Eulerâ€™s number, approximately $2.718$.\n",
    "- $\\sigma(z)$ = the output of the sigmoid function, which is a value between 0 and 1.\n",
    "\n",
    "### **Interpretation:**\n",
    "- The sigmoid function maps any real-valued number to a value between 0 and 1. This makes it suitable for predicting probabilities.\n",
    "- The closer $\\sigma(z)$ is to 1, the more likely the positive class (1) is. The closer it is to 0, the more likely the negative class (0) is.\n",
    "\n",
    "## 3. Hypothesis for Logistic Regression\n",
    "- In **Logistic Regression**, instead of directly predicting $y$ as in **Linear Regression**, we predict the probability of $y$ being 1, given the input features.\n",
    "\n",
    "### **Hypothesis Equation:**\n",
    "$\n",
    "h_\\theta(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\cdot x_1 + \\dots + \\beta_n \\cdot x_n)}}$\n",
    "\n",
    "Where:\n",
    "- $h_\\theta(x)$ = predicted probability that $y = 1$.\n",
    "- $\\beta_0, \\beta_1, \\dots, \\beta_n$ = coefficients of the model.\n",
    "- $x_1, x_2, \\dots, x_n$ = independent variables (features).\n",
    "\n",
    "### **Thresholding:**\n",
    "- To make predictions, we can apply a threshold:\n",
    "  - If $h_\\theta(x) \\geq 0.5$, predict class 1.\n",
    "  - If $h_\\theta(x) < 0.5$, predict class 0.\n",
    "\n",
    "## 4. Cost Function for Logistic Regression\n",
    "- The cost function used in **Logistic Regression** is different from **Linear Regression** because we are dealing with probabilities. Instead of using Mean Squared Error (MSE), **Logistic Regression** uses the **log-loss** or **cross-entropy loss**.\n",
    "\n",
    "### **Log-Loss (Cost Function):**\n",
    "$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\right]\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $m$ = number of training examples.\n",
    "- $y_i$ = actual label (0 or 1) for the $i^{th}$ training example.\n",
    "- $h_\\theta(x_i)$ = predicted probability for the $i^{th}$ training example.\n",
    "\n",
    "### **Explanation:**\n",
    "- When the actual label $y = 1$, the first term $y_i \\log(h_\\theta(x_i))$ dominates the cost.\n",
    "- When the actual label $y = 0$, the second term $(1 - y_i) \\log(1 - h_\\theta(x_i))$ dominates the cost.\n",
    "- This cost function penalizes incorrect predictions heavily, especially when the predicted probability is far from the actual label.\n",
    "\n",
    "\n",
    "## 5. Gradient Descent in Logistic Regression\n",
    "- Just like in **Linear Regression**, we can use **Gradient Descent** to minimize the cost function and find the optimal parameters $\\beta_0, \\beta_1, \\dots, \\beta_n$.\n",
    "- The key difference is that in **Logistic Regression**, the cost function is non-linear, but **Gradient Descent** still works effectively.\n",
    "\n",
    "### **Gradient Descent Update Rule:**\n",
    "For each parameter $\\beta_j$, we update it as follows:\n",
    "$\n",
    "\\beta_j = \\beta_j - \\alpha \\cdot \\frac{\\partial}{\\partial \\beta_j} J(\\beta)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = learning rate (controls the step size in each iteration).\n",
    "- $J(\\beta)$ = cost function (log-loss).\n",
    "- $\\frac{\\partial}{\\partial \\beta_j} J(\\beta)$ = partial derivative of the cost function with respect to $\\beta_j$.\n",
    "\n",
    "### **Gradient Descent for Logistic Regression:**\n",
    "For Logistic Regression, the partial derivatives are calculated as:\n",
    "$\n",
    "\\frac{\\partial}{\\partial \\beta_j} J(\\beta) = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x_i) - y_i) \\cdot x_{ij}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $x_{ij}$ is the $j^{th}$ feature of the $i^{th}$ training example.\n",
    "\n",
    "## 6. Key Concepts\n",
    "\n",
    "### **Decision Boundary:**\n",
    "- The **decision boundary** is a threshold that separates the two classes (0 and 1). In Logistic Regression, this boundary is linear if there is only one feature, and it can become more complex with multiple features.\n",
    "- The decision boundary is where the predicted probability $h_\\theta(x) = 0.5$.\n",
    "\n",
    "### **Overfitting and Underfitting:**\n",
    "- Like other machine learning models, **Logistic Regression** can suffer from **overfitting** (when the model fits the training data too well and performs poorly on unseen data) and **underfitting** (when the model is too simple to capture the relationship between the features and the target).\n",
    "\n",
    "### **Regularization:**\n",
    "- **Regularization** techniques like **L2 regularization (Ridge)** or **L1 regularization (Lasso)** can help prevent overfitting by adding a penalty term to the cost function.\n",
    "- **L2 regularization** adds the sum of squared coefficients to the cost function:\n",
    "  $\n",
    "  J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\beta_j^2\n",
    "  $\n",
    "- **L1 regularization** adds the absolute values of the coefficients:\n",
    "  $\n",
    "  J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)) \\right] + \\frac{\\lambda}{m} \\sum_{j=1}^{n} |\\beta_j|\n",
    "  $\n",
    "- $\\lambda$ is a hyperparameter that controls the amount of regularization.\n",
    "\n",
    "## 7. Conclusion\n",
    "- **Logistic Regression** is a simple yet powerful algorithm for binary classification problems. It is widely used because of its interpretability and efficiency.\n",
    "- The **sigmoid function** enables us to model probabilities, and the **log-loss** cost function ensures that the model focuses on minimizing classification errors.\n",
    "- **Gradient Descent** helps optimize the model parameters, and regularization techniques can be applied to prevent overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6df51e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T15:05:54.678556Z",
     "iopub.status.busy": "2024-09-12T15:05:54.678012Z",
     "iopub.status.idle": "2024-09-12T15:05:54.750474Z",
     "shell.execute_reply": "2024-09-12T15:05:54.749324Z"
    },
    "papermill": {
     "duration": 0.079133,
     "end_time": "2024-09-12T15:05:54.753151",
     "exception": false,
     "start_time": "2024-09-12T15:05:54.674018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_project</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>Work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>Department</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_project  average_montly_hours  \\\n",
       "0                0.38             0.53               2                   157   \n",
       "1                0.80             0.86               5                   262   \n",
       "2                0.11             0.88               7                   272   \n",
       "3                0.72             0.87               5                   223   \n",
       "4                0.37             0.52               2                   159   \n",
       "\n",
       "   time_spend_company  Work_accident  left  promotion_last_5years Department  \\\n",
       "0                   3              0     1                      0      sales   \n",
       "1                   6              0     1                      0      sales   \n",
       "2                   4              0     1                      0      sales   \n",
       "3                   5              0     1                      0      sales   \n",
       "4                   3              0     1                      0      sales   \n",
       "\n",
       "   salary  \n",
       "0     low  \n",
       "1  medium  \n",
       "2  medium  \n",
       "3     low  \n",
       "4     low  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "hr_data = pd.read_csv('/kaggle/input/hr-data/HR_comma_sep.csv')\n",
    "hr_data.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5687345,
     "sourceId": 9376105,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.114979,
   "end_time": "2024-09-12T15:05:55.278022",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-12T15:05:51.163043",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
